{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model for Video Prediction\n",
    "\n",
    "This notebook demonstrates the use of a deep learning model to predict outcomes based on video input. It includes steps from setting up the environment to processing video data for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-29T03:14:31.423002Z",
     "iopub.status.busy": "2024-04-29T03:14:31.422567Z",
     "iopub.status.idle": "2024-04-29T03:14:44.954855Z",
     "shell.execute_reply": "2024-04-29T03:14:44.953671Z",
     "shell.execute_reply.started": "2024-04-29T03:14:31.422969Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        pass\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "The following libraries are necessary for face recognition and image processing. They provide the tools required to manipulate video and image data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T10:48:11.423101Z",
     "iopub.status.busy": "2024-04-29T10:48:11.422016Z",
     "iopub.status.idle": "2024-04-29T10:49:18.302056Z",
     "shell.execute_reply": "2024-04-29T10:49:18.300576Z",
     "shell.execute_reply.started": "2024-04-29T10:48:11.423062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting face_recognition\n",
      "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
      "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from face_recognition) (8.1.7)\n",
      "Collecting dlib>=19.7 (from face_recognition)\n",
      "  Downloading dlib-19.24.4.tar.gz (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from face_recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from face_recognition) (9.5.0)\n",
      "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: dlib, face-recognition-models\n",
      "  Building wheel for dlib (pyproject.toml) ... \u001b[?25l|^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install face_recognition\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mface_recognition\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "!pip install face_recognition\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Here we import necessary libraries and define any initial setup configurations. This includes setting up the environment and importing various modules needed throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding for Reproducibility\n",
    "\n",
    "To ensure that our experiments can be replicated, we set a seed which makes all random operations deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T10:49:18.303010Z",
     "iopub.status.idle": "2024-04-29T10:49:18.303439Z",
     "shell.execute_reply": "2024-04-29T10:49:18.303262Z",
     "shell.execute_reply.started": "2024-04-29T10:49:18.303244Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "This section defines the architecture of our deep learning model. The model is designed to classify images based on multiple labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Here, we initialize the model with the predefined architecture and prepare it for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T03:15:59.640040Z",
     "iopub.status.busy": "2024-04-29T03:15:59.638970Z",
     "iopub.status.idle": "2024-04-29T03:15:59.655295Z",
     "shell.execute_reply": "2024-04-29T03:15:59.653793Z",
     "shell.execute_reply.started": "2024-04-29T03:15:59.639999Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED =  0\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "is_gpu_available = torch.cuda.is_available()\n",
    "print(f'Is using GPU: {is_gpu_available}')\n",
    "device = torch.device('cuda' if is_gpu_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing\n",
    "\n",
    "We define the transformations that will be applied to the images for normalization and augmentation. These are critical for preparing the data for processing by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T03:16:04.520589Z",
     "iopub.status.busy": "2024-04-29T03:16:04.520143Z",
     "iopub.status.idle": "2024-04-29T03:16:04.529628Z",
     "shell.execute_reply": "2024-04-29T03:16:04.527848Z",
     "shell.execute_reply.started": "2024-04-29T03:16:04.520553Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLabelImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLabelImageClassifier, self).__init__()\n",
    "        self.base_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.base_model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection Function\n",
    "\n",
    "This function is responsible for detecting and extracting faces from images. It uses pre-trained models from the `face_recognition` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T03:16:07.405635Z",
     "iopub.status.busy": "2024-04-29T03:16:07.404519Z",
     "iopub.status.idle": "2024-04-29T03:16:12.116545Z",
     "shell.execute_reply": "2024-04-29T03:16:12.115183Z",
     "shell.execute_reply.started": "2024-04-29T03:16:07.405567Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MultiLabelImageClassifier(num_classes=2).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video to Image Conversion\n",
    "\n",
    "This function converts video files into a list of image frames. This is essential for processing video data where each frame is treated as a separate input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T09:52:56.526774Z",
     "iopub.status.busy": "2024-04-28T09:52:56.526361Z",
     "iopub.status.idle": "2024-04-28T09:52:56.534467Z",
     "shell.execute_reply": "2024-04-28T09:52:56.532970Z",
     "shell.execute_reply.started": "2024-04-28T09:52:56.526743Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Settings\n",
    "\n",
    "Here we set various parameters that will be used in the model and data processing. These include frame count thresholds and other relevant settings for the video processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T09:52:56.734787Z",
     "iopub.status.busy": "2024-04-28T09:52:56.734313Z",
     "iopub.status.idle": "2024-04-28T09:52:56.742678Z",
     "shell.execute_reply": "2024-04-28T09:52:56.741086Z",
     "shell.execute_reply.started": "2024-04-28T09:52:56.734736Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_face_img(image):\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "    for _, face_location in enumerate(face_locations):\n",
    "        top, right, bottom, left = face_location\n",
    "        face_image = image[top:bottom, left:right]\n",
    "        \n",
    "        face_image = Image.fromarray(face_image)\n",
    "        \n",
    "        return transform(face_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T09:52:57.306683Z",
     "iopub.status.busy": "2024-04-28T09:52:57.306243Z",
     "iopub.status.idle": "2024-04-28T09:52:57.315193Z",
     "shell.execute_reply": "2024-04-28T09:52:57.313890Z",
     "shell.execute_reply.started": "2024-04-28T09:52:57.306640Z"
    }
   },
   "outputs": [],
   "source": [
    "def video_to_image_list(video_path, frame_count):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    image_list = []\n",
    "    cnt = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cnt += 1\n",
    "\n",
    "        if not ret or cnt > frame_count:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = get_face_img(frame)\n",
    "        if frame != None:\n",
    "            image_list.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return torch.stack(image_list, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Testing\n",
    "\n",
    "After training, we evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T09:52:57.848898Z",
     "iopub.status.busy": "2024-04-28T09:52:57.848445Z",
     "iopub.status.idle": "2024-04-28T09:52:57.860285Z",
     "shell.execute_reply": "2024-04-28T09:52:57.858911Z",
     "shell.execute_reply.started": "2024-04-28T09:52:57.848861Z"
    }
   },
   "outputs": [],
   "source": [
    "FRAME_COUNT = 15\n",
    "THRESHOLD = (FRAME_COUNT//2)+1\n",
    "\n",
    "def test_video(video_path_lst, is_df):\n",
    "    \n",
    "    video_cnt = 0\n",
    "    class_target = 0\n",
    "    if is_df:\n",
    "        class_target = 1\n",
    "    \n",
    "    correct_cnt = 0\n",
    "    for video_path in video_path_lst:\n",
    "        if video_cnt % 10 == 0:\n",
    "            print(\"Done\", video_cnt, \"videos\")\n",
    "\n",
    "        img_list = video_to_image_list(video_path, FRAME_COUNT)\n",
    "        pred_output = model(img_list)\n",
    "        \n",
    "        _, pred_class_img = pred_output.topk(1, dim=1)\n",
    "        \n",
    "        pred_class_vid = 0\n",
    "        if sum(pred_class_img) >= THRESHOLD:\n",
    "            pred_class_vid = 1\n",
    "        \n",
    "        video_cnt += 1\n",
    "        if pred_class_vid == class_target:\n",
    "            correct_cnt += 1\n",
    "        \n",
    "    return [correct_cnt, video_cnt-correct_cnt]\n",
    "\n",
    "def count_acc(df_stat, real_stat):\n",
    "    total_vid = sum(df_stat)+sum(real_stat)\n",
    "    \n",
    "    pred_a = df_stat[0]+real_stat[0]\n",
    "    pred_b = df_stat[1]+real_stat[1]\n",
    "\n",
    "    if pred_a > pred_b:\n",
    "        print(\"Accuracy deepfake:\", df_stat[0]/sum(df_stat)*100, \"%\")\n",
    "        print(\"Accuracy real video:\", real_stat[0]/sum(real_stat)*100, \"%\")\n",
    "        return pred_a/total_vid\n",
    "    \n",
    "    print(\"Accuracy deepfake:\", df_stat[1]/sum(df_stat)*100, \"%\")\n",
    "    print(\"Accuracy real video:\", real_stat[1]/sum(real_stat)*100, \"%\")\n",
    "    return pred_b/total_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/kaggle/input/deepfake-detection/pytorch/cosine_5e-3/1/epoch39.pth\"\n",
    "model = MultiLabelImageClassifier(num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Predicting deepfake...\")\n",
    "video_list = glob.glob(\"/kaggle/input/celeb-df-deep-learning/Celeb-DF-v2_separated/Celeb-DF-v2_separated/test/deepfake/*\")\n",
    "df_stat = test_video(video_list, is_df=True)\n",
    "\n",
    "print(\"Predicting real...\")\n",
    "video_list = glob.glob(\"/kaggle/input/celeb-df-deep-learning/Celeb-DF-v2_separated/Celeb-DF-v2_separated/test/real/*\")\n",
    "real_stat = test_video(video_list, is_df=False)\n",
    "\n",
    "acc = count_acc(df_stat, real_stat)\n",
    "print(\"Average accuracy:\", acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4892144,
     "sourceId": 8246131,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 34353,
     "sourceId": 40806,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 34275,
     "sourceId": 40808,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 34273,
     "sourceId": 40846,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 34377,
     "sourceId": 40847,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 34591,
     "sourceId": 41110,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
